{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <header>\n",
    "        <h1>Audio Classifier Python</h1>\n",
    "        <h2>Marco Bondaschi, Mauro Conte</h2>\n",
    "        <h4>Laboratorio di Telecomunicazioni</h4>\n",
    "        <h5>Università Degli Studi Di Brescia</h5>\n",
    "        <h6>13/03/2017</h6>\n",
    "    </header>\n",
    "</center>\n",
    "<hr><hr>\n",
    "\n",
    "<h2>Descrizione</h2>\n",
    "\n",
    "Per questa prima esperienza di laboratorio, abbiamo realizzato un software di classificazione audio, realizzato in Python, che permettesse di analizzare e distinguere un file musicale da uno di parlato.\n",
    "\n",
    "Per realizzare questo compito abbiamo usufruito di un database di 40 file audio .wav, MONO e con frequenza di campionamento pari a 16000 Hz, già suddivisi in 20 file di musica (dalla durata di 10 s l'uno) e 20 file di parlato (dalla durata di 3 s l'uno).\n",
    "\n",
    "Abbiamo inoltre in dotazione un database di 10 suoni incogniti, sempre MONO a 16000Hz, che il nostro software dovrà essere in grado di dividere in musica o parlato. A questi 10 file abbiamo ne abbiamo aggiunti altri due, uno di musica e uno di parlato, per verificare con più precisione l'accuratezza del programma.\n",
    "\n",
    "Il primo passo per creare il software di classificazione è la fase di training, attraverso la quale si addestra il software a distinguere musica e parlato facendogli analizzare i 40 file già suddivisi nelle due categorie.\n",
    "\n",
    "In secondo luogo si attua la fase di test, in cui si consegnano al software i 10 file incogniti, che verranno analizzati dal programma e suddivisi in musica o parlato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Strumenti</h3>\n",
    "\n",
    "Per raggiungere questi obiettivi, è stato necessario utilizzare le estensioni Scipy e Numpy, che implementano funzioni matematiche di alto livello. In particolare abbiamo utilizzato due funzioni fondamentali: spectrogram e svd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import scipy.linalg\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spectrogram</h3>\n",
    "La prima serve per ottenere lo spettrogramma di un segnale audio, che consiste nel suddividere il segnale in intervalli di tempo uguali, finestrarli utilizzando una delle numerose finestre disponibili (nel nostro caso la finestra di Hamming), quindi calcolare la FFT di questi segmenti, utilizzando un numero di campioni a scelta. Da questo procedimento si ottiene perciò una matrice, dove ogni colonna contiene la FFT del segmento di segnale designato (e dunque il numero di righe della matrice equivale al numero di campioni della FFT, mentre il numero di colonne equivale al numero di segmenti in cui è stato suddiviso il segnale).\n",
    "\n",
    "<h3>SVD</h3>\n",
    "La seconda funzione, invece, esegue la Decomposizione ai Valori Singolari (SVD, Singular Value Decomposition) su una matrice $M\\in{\\mathbb{C}^{(m,n)}}$\n",
    "\n",
    "Questa pratica consiste in una fattorizzazione della matrice $M$ in un prodotto tra tre matrici $M=USV^{*}$, dove:\n",
    "<ul>\n",
    "    <li>\n",
    "$U\\in{\\mathbb{C}^{(m,n)}}$ è una matrice unitaria (ovvero una matrice il cui prodotto con la sua matrice trasposta coniugata dà la matrice unitaria)\n",
    "    </li>\n",
    "    <li>\n",
    "$S$ è una matrice diagonale  $\\in\\mathbb{C}^{(m,n)}$\n",
    "    </li>\n",
    "    <li>\n",
    "$V^{*}$ è la trasposta coniugata di una matrice unitaria $V\\in{\\mathbb{C}^{(n,n)}}$\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "Gli elementi sulla diagonale principale di $S$ sono le radici quadrate degli autovalori associati agli autovettori che si trovano sulle colonne di $V^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Funzioni implementate</h3>\n",
    "\n",
    "Ritorna una lista di tuple contenenti SampleRate, Data, FileName data una path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        rate, data = scipy.io.wavfile.read(path + filename) \n",
    "        audio_list.append((rate, data, filename))\n",
    "    return audio_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ritorna lo spettrogramma  di un segnale dati il SampleRate e Data.\n",
    "\n",
    "Come impostazioni per lo spettrogramma abbiamo seguito la documentazione dello standard mpeg7 allegata, paragrafo 1.1.2.3.4. Poichè la frequenza di campionamento dei file è pari a 16000 Hz, abbiamo ottenuto una hopsize (corrispondente al valore noverlap della funzione di Numpy) pari a 160, la lunghezza della finestra pari alla più piccola potenza di 2 maggiore di 3 hopsize, ovvero 512 campioni, che è anche il numero di campioni della FFT. La finestra utilizzata, come da specifiche, è quella di Hamming.\n",
    "Inolte, abbiamo calcolato lo spettrogramma in scala logaritmica in modo da velocizzare le operazioni computazionali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spectrogram(data, rate):\n",
    "    f, t, sxx = scipy.signal.spectrogram(data, rate, 'hamming', 512, 160, 512)\n",
    "    spectrogram = 10 * np.log10(sxx + sys.float_info.min)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ritorna lo spettrogramma di un insieme di audio data la lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spectrogram_from_list(collection):\n",
    "    spectrogram = None\n",
    "    for rate,data,f_name in collection:\n",
    "        if spectrogram is None: spectrogram = get_spectrogram(data,rate)\n",
    "        else: scipy.hstack((spectrogram, get_spectrogram(data,rate)))\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ritorna una matrice che ha per colonne i vettori di una base ridotta dello spazio vettoriale che genera lo spettrogramma dato lo stesso e la percentuale di energia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_from_spectrogram(spectrogram,percent):\n",
    "    u, s, v = np.linalg.svd(spectrogram)\n",
    "    energy_e_value = np.sum(s)\n",
    "    current_sum = 0\n",
    "    percent_sum = energy_e_value * percent / 100\n",
    "    for i in range(len(s)):\n",
    "        current_sum += s[i]\n",
    "        if current_sum >= percent_sum:\n",
    "            break\n",
    "    base = u[:, range(0, i)]\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ritorna la feature relativa ad un audio ricavata dal suo spettrogramma e dalla base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_mean(spectrogram, base):\n",
    "    mean_vector = np.zeros(base.shape[1])\n",
    "    coefficient_on_base = np.dot(spectrogram.T, base)\n",
    "    mean_vector += np.mean(coefficient_on_base, 0)\n",
    "    return mean_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ritorna la feature relativa ad una lista di audio ricavata dal suo spettrogramma e dalla base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_mean_from_list(collection, base):\n",
    "    mean_vector = np.zeros(base.shape[1])\n",
    "    for rate, data, filename in collection:\n",
    "        spectrogram = get_spectrogram(data,rate)\n",
    "        mean_vector += get_feature_mean(spectrogram,base)\n",
    "    return mean_vector / len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcola l'approssimazione dello spettrogramma su una base, quindi ritorna l'errore tra lo spettrogramma originale e l'approssimato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_error_base(spectrogram, base):\n",
    "    coefficient_on_base = np.dot(spectrogram.T, base)\n",
    "    spectrogram_ric = np.dot(coefficient_on_base, base.T).T\n",
    "    error = np.linalg.norm(spectrogram - spectrogram_ric)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Per implementare il training abbiamo utilizzato tre diversi metodi.\n",
    "\n",
    "<h3>Primo Metodo</h3>\n",
    "\n",
    "Innanzitutto abbiamo caricato i file audio di musica e di parlato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_db = os.getcwd()+'/05_AudioClassifier_Pdf/05_AudioClassifier_Pdf/database/'\n",
    "path_music = path_db + 'music/'\n",
    "path_speech = path_db + 'speech/'\n",
    "path_unknowns = path_db.replace('database/', '') + 'unknownSounds/'\n",
    "\n",
    "music = load_audio(path_music)\n",
    "speech = load_audio(path_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quindi abbiamo calcolato lo spettrogramma per ogni file, utilizzando le impostazioni descritte in precedenza.\n",
    "A questo punto abbiamo concatenato tra loro tutti gli spettrogrammi dei file di musica, e poi abbiamo ripetuto la stessa cosa per i file di parlato, in modo da ottenere due classi distinte che rappresentassero le due categorie. Algebricamente parlando, le due classi risultano essere due sottospazi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectrogram_music = get_spectrogram_from_list(music)\n",
    "spectrogram_speech = get_spectrogram_from_list(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo è stato quello di ottenere due basi di vettori che generassero i sottospazi; abbiamo perciò calcolato la SVD dei due spettrogrammi, ricavando dalle colonne della matrice U gli autovettori richiesti, con l'accortezza di non sceglierli tutti, in quanto così facendo avremmo potuto ottenere ricostruzioni perfette di ogni segnale per entrambi i sottospazi, impedendo una classificazione efficace. Al contrario, poichè ad ogni autovettore è associato un autovalore che rappresenta la sua energia, abbiamo selezionato solo i primi autovettori in ordine di energia, affinchè la somma dei loro autovalori fosse pari al 50% del totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "energy = 50\n",
    "base_music = get_base_from_spectrogram(spectrogram_music,energy)\n",
    "base_speech = get_base_from_spectrogram(spectrogram_speech,energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta ottenute le due basi, abbiamo caricato i 10 file incogniti, e per ognuno di essi abbiamo calcolato lo spettrogramma (con le impostazioni precedenti). Successivamente abbiamo calcolato per ognuno di essi la proiezione dello spettrogramma su entrambi i sottospazi attraverso i prodotti scalari tra le colonne dello spettrogramma e gli autovettori della classe. A questo punto abbiamo calcolato le due matrici errore come la differenza tra la proiezione del file incognito su un sottospazio e lo spettrogramma originale, e ne abbiamo calcolato le norme quadratiche (o di Frobenius); la categoria la cui matrice errore aveva norma minore era quella che meglio rappresentava il segnale incognito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  007si1079.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  007si1271.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  014si1291.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  016si1621.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo1::  PARLATO\n"
     ]
    }
   ],
   "source": [
    "unknowns = load_audio(path_unknowns)\n",
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = get_error_base(current_spectrogram,base_speech)\n",
    "    err_on_music = get_error_base(current_spectrogram,base_music)\n",
    "    print('\\tMetodo1:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Secondo metodo</h3>\n",
    "\n",
    "Per il secondo metodo si procede allo stesso modo del precedente fino ad ottenere le due basi di autovettori per le classi di musica e di parlato.\n",
    "A questo punto, invece che procedere con la proiezione dei suoni incogniti, si ottiene una 'feature' per ognuna delle due classi: una 'feature' è un elemento che rappresenta in modo sintetico l'intera classe. Per ottenere questa 'feature' abbiamo calcolato il vettore dei coefficienti della proiezione di ognuno dei vettori dello spettrogramma di una classe musica sulla base di autovettori della stessa classe, e poi abbiamo calcolato il vettore medio tra tutti i vettori dei coefficienti. Questi due vettori medi, uno per classe, rappresentano le nostre 'feature' per le due categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mean_music = get_feature_mean_from_list(music,base_music)\n",
    "feature_mean_speech = get_feature_mean_from_list(speech,base_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passando quindi ai file incogniti, abbiamo calcolato lo spettrogramma di ognuno, e abbiamo proceduto come con lo spettrogramma delle due classi, calcolandone il vettore medio dei coefficienti. Quindi, per decidere quale categoria è quella corretta, abbiamo calcolato i due vettori errore come differenza tra il vettore medio del file incognito e la 'feature' della classe, e quello che possedeva norma quadratica minore era quello della classe giusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  007si1079.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  007si1271.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  014si1291.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  016si1621.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo2::  PARLATO\n"
     ]
    }
   ],
   "source": [
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = np.linalg.norm(feature_mean_speech-get_feature_mean(current_spectrogram,base_speech))\n",
    "    err_on_music = np.linalg.norm(feature_mean_music-get_feature_mean(current_spectrogram,base_music))\n",
    "    print('\\tMetodo2:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Terzo metodo</h3>\n",
    "\n",
    "Per il terzo metodo si procede sulla falsa riga del secondo, ma al contrario di quest'ultimo si utilizza una sola classe che include sia i file di musica che quelli di parlato. In pratica si concatenano gli spettrogrammi delle due categorie calcolati in precedenza, in modo da formare un unico spettrogramma, da cui si ricava la base di autovettori tramite la SVD nello stesso modo con cui la si era ottenuta per le singole classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spectrogram_total = scipy.hstack((spectrogram_music, spectrogram_speech))\n",
    "\n",
    "energy = 50\n",
    "base_total = get_base_from_spectrogram(spectrogram_total,energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, si ottengono le 'feature' delle due categorie calcolando i vettori dei coefficienti della proiezione di ogni file di una delle due categorie sulla classe unica, e calcolandone il vettore medio (come fatto anche nel secondo metodo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mean_music_on_total = get_feature_mean_from_list(music,base_total)\n",
    "feature_mean_speech_on_total = get_feature_mean_from_list(speech,base_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede quindi con i file incogniti, calcolandone come prima il vettore medio dei coefficienti della proiezione sulla classe unica. Per decidere quale categoria è quella corretta, si calcolano i due vettori errore come differenza tra il vettore medio del file incognito e la 'feature' della categoria, prediligendo quello che possiede norma quadratica minore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  007si1079.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  007si1271.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  014si1291.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  016si1621.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo3::  PARLATO\n"
     ]
    }
   ],
   "source": [
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = np.linalg.norm(feature_mean_speech_on_total-get_feature_mean(current_spectrogram,base_total))\n",
    "    err_on_music = np.linalg.norm(feature_mean_music_on_total-get_feature_mean(current_spectrogram,base_total))\n",
    "    print('\\tMetodo3:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analisi dei risultati</h3>\n",
    "\n",
    "Per quanto riguarda il primo metodo, utilizzando il 50% dell'energia nel calcolo della base di autovettori, si ottiene una classificazione corretta per tutti i file audio eccetto il primo. La causa di questo fatto è dovuta probabilmente alla natura acustica del brano, in cui il cantante è accompagnato dalla sola chitarra. Questa particolarità, che non si riscontra negli altri brani musicali della collezione, rende l'audio più simile al parlato e trae in inganno il sistema di classificazione.\n",
    "Provando a diminuire il livello di energia, i risultati restano uguali. Ad esempio, con una percentuale del 30% si ottiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  007si1079.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  007si1271.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  014si1291.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  016si1621.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo1::  PARLATO\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo1::  PARLATO\n"
     ]
    }
   ],
   "source": [
    "energy = 30\n",
    "base_music = get_base_from_spectrogram(spectrogram_music,energy)\n",
    "base_speech = get_base_from_spectrogram(spectrogram_speech,energy)\n",
    "\n",
    "unknowns = load_audio(path_unknowns)\n",
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = get_error_base(current_spectrogram,base_speech)\n",
    "    err_on_music = get_error_base(current_spectrogram,base_music)\n",
    "    print('\\tMetodo1:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al contrario, aumentando il livello di energia i risultati sono via via meno precisi, tendendo a riconoscere come musica anche gli audio di parlato. Ad esempio, con l'80% di energia risulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  007si1079.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  007si1271.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  014si1291.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  016si1621.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo1::  MUSICA\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo1::  MUSICA\n"
     ]
    }
   ],
   "source": [
    "energy = 80\n",
    "base_music = get_base_from_spectrogram(spectrogram_music,energy)\n",
    "base_speech = get_base_from_spectrogram(spectrogram_speech,energy)\n",
    "\n",
    "unknowns = load_audio(path_unknowns)\n",
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = get_error_base(current_spectrogram,base_speech)\n",
    "    err_on_music = get_error_base(current_spectrogram,base_music)\n",
    "    print('\\tMetodo1:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il problema di prendere un numero così elevato di autovettori è quello che entrambi gli spazi vettoriali tendono a ricostruire perfettamente tutti i segnali (cosa che avverrebbe perfettamente nel caso di una base completa), inficiando i risultati del programma.\n",
    "\n",
    "Il secondo metodo risulta essere molto più solido rispetto al valore di energia scelto, e riconosce in maniera corretta tutti gli audio anche aumentando il livello di energia. Ad esempio, con il 100% dell'energia si ottiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  007si1079.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  007si1271.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  014si1291.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  016si1621.wav\n",
      "\tMetodo2::  PARLATO\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo2::  MUSICA\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo2::  PARLATO\n"
     ]
    }
   ],
   "source": [
    "energy = 100\n",
    "base_music = get_base_from_spectrogram(spectrogram_music,energy)\n",
    "base_speech = get_base_from_spectrogram(spectrogram_speech,energy)\n",
    "\n",
    "feature_mean_music = get_feature_mean_from_list(music,base_music)\n",
    "feature_mean_speech = get_feature_mean_from_list(speech,base_speech)\n",
    "\n",
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = np.linalg.norm(feature_mean_speech-get_feature_mean(current_spectrogram,base_speech))\n",
    "    err_on_music = np.linalg.norm(feature_mean_music-get_feature_mean(current_spectrogram,base_music))\n",
    "    print('\\tMetodo2:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo stesso discorso vale per il terzo metodo, in linea teorica il più robusto. Infatti, con il 100% dell'energia le classificazioni risultano ancora tutte corrette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  004_BobDylan-OxfordTown-10s-B.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  007si1079.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  007si1271.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  010_CiboMatto-Moonchild-10s-A.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  014si1291.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  016si1621.wav\n",
      "\tMetodo3::  PARLATO\n",
      "Item:  019_Nirvana-Downer-10s-B.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  024_JimiHendrix-WaitUntilTomorrow-10s-A.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  025_Lana-SadGirl.wav\n",
      "\tMetodo3::  MUSICA\n",
      "Item:  026_agnel.wav\n",
      "\tMetodo3::  PARLATO\n"
     ]
    }
   ],
   "source": [
    "energy = 100\n",
    "base_total = get_base_from_spectrogram(spectrogram_total,energy)\n",
    "\n",
    "feature_mean_music_on_total = get_feature_mean_from_list(music,base_total)\n",
    "feature_mean_speech_on_total = get_feature_mean_from_list(speech,base_total)\n",
    "\n",
    "for rate,data,filename in unknowns:\n",
    "    current_spectrogram = get_spectrogram(data , rate)\n",
    "    print('Item: ',filename)\n",
    "    \n",
    "    err_on_speech = np.linalg.norm(feature_mean_speech_on_total-get_feature_mean(current_spectrogram,base_total))\n",
    "    err_on_music = np.linalg.norm(feature_mean_music_on_total-get_feature_mean(current_spectrogram,base_total))\n",
    "    print('\\tMetodo3:: ','MUSICA' if err_on_music<err_on_speech else'PARLATO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusioni</h3>\n",
    "\n",
    "In conclusione, i risultati confermano la buona riuscita dell'esperienza, che con due metodi su tre permette un perfetto riconoscimento degli audio, indipendentemente dalla percentuale di energia utilizzata nel formare la base delle due classi musica e parlato. \n",
    "Al contrario degli ultimi due, il primo metodo risulta invece meno preciso, dando la migliore classificazione con il 50% di energia: con questa impostazione il risultato è corretto per 11 file su 12, con l'unico errore per la canzone di Bob Dylan, acustica e quindi con caratteristiche simili al parlato.\n",
    "La migliore riuscita degli ultimi due metodi è da imputare al fatto che le feature risultano molto diverse tra loro, ed è quindi più semplice per il programma distinguere a quale delle due classi appartiene un audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
